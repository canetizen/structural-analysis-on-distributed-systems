# Structural Analysis of Publish–Subscribe Architectures

This repository contains the **research prototype, synthetic datasets, and
expert evaluation framework** used to **experimentally validate the metric
formulation, combined scoring model, and expert consistency** proposed in
the following academic study:

> **Yayınla–Abone Ol Tabanlı Dağıtık Sistemlerde Yapısal Etkileşim  
> Örüntülerinin Çizge Tabanlı Statik Analiz ile İncelenmesi**  
> *A Graph-Based Static Analysis of Structural Interaction Patterns  
> in Publish–Subscribe Based Distributed Systems*  
>  
> UYMS 2026 (under review / submitted)

**Important:**  
This repository does **not** implement static analysis or CodeQL extraction.
It assumes that architectural relationships are already available as input.

---

## Purpose

The goal of this repository is to **evaluate and stress-test the proposed
structural metric definitions and combined anomaly scoring model** under
different architectural scenarios, and to **compare analysis results with
expert evaluations** using majority voting and standard IR metrics.

Specifically, it aims to:

- apply the **formal metric definitions** presented in the paper,
- evaluate **rule-based structural patterns** derived from relative metric behavior,
- compute the **combined anomaly score** used for prioritization,
- generate **expert evaluation templates** for each dataset,
- compare system rankings with expert assessments using **Precision@K, Recall@K, F1@K**,
- analyze what types of architectural situations are captured or missed
  by the scoring model.

The analysis is intentionally limited to **relative ranking**, not defect
detection or classification.

---

## Scope and Assumptions

- Input data represents an **already-extracted architectural graph**
  (applications, topics, nodes, libraries).
- No source code parsing, static analysis, or runtime data is involved.
- All thresholds are **system-relative** (quartile-based).
- Results are meaningful only **within the same system context**.

---

## Repository Structure

```
├── structural_analysis.py        # Main analysis tool
├── compare_expert.py             # Expert evaluation comparison tool
├── Makefile                      # Build & run commands
├── datasets/                     # Synthetic architectural graphs (JSON)
│   ├── hub_application.json
│   ├── single_backbone_topic.json
│   ├── context_diversity_comparison.json
│   └── single_metric_outlier.json
├── results/                      # Analysis output (generated)
│   ├── hub_application_results.txt
│   └── ...
└── experts/                      # Expert evaluations (per dataset)
    ├── hub_application/
    │   ├── template.txt          # Generated by structural_analysis.py
    │   ├── expert_1.txt          # Filled by experts
    │   └── expert_2.txt
    └── ...
```

---

## Methodology Overview

The implementation directly follows the **formulation layer** of the paper:

1. **Structural Metrics**
   - Metrics are computed at application, topic, node, and library levels
   - Definitions correspond exactly to the mathematical formulations

2. **Relative Interpretation**
   - Metrics are interpreted using system-wide distributions (Q1 / Q3)
   - No absolute thresholds are used

3. **Rule-Based Structural Patterns**
   - Multiple metrics are combined into interpretable architectural patterns
   - Single-metric dominance is explicitly avoided

4. **Combined Anomaly Score**
   - Pattern-based anomaly score
   - Limited single-metric contribution
   - Used solely for **relative prioritization**

5. **Expert Evaluation**
   - Per-dataset expert templates are auto-generated
   - Experts mark components as structurally atypical (E) or normal (H)
   - Majority voting (≥3 out of 5) determines ground truth
   - Results compared via Precision@K, Recall@K, F1@K (K=5, 10)

---

## Datasets

The `datasets/` directory contains **small synthetic architectural graphs**
designed to simulate different structural scenarios discussed in the paper
(e.g., dominant hubs, communication backbones, node-level concentration,
single-metric outliers).

These datasets are **illustrative**, not realistic production systems.

---

## Running the Analysis

### Requirements
- Python **3.9+**
- `pandas`

### Quick Start

```bash
# Run analysis and generate expert templates
make analyze

# Compare expert evaluations (single dataset)
make compare DATASET=hub_application

# Compare all datasets with detailed output
make compare-all-detailed

# Clean all generated files
make clean
```

### All Available Commands

| Command | Description |
|---------|-------------|
| `make analyze` | Run analysis on all datasets, generate results and expert templates |
| `make compare DATASET=<name>` | Compare single dataset with expert evaluations |
| `make compare-detailed DATASET=<name>` | Same with detailed per-component output |
| `make compare-all` | Compare all datasets |
| `make compare-all-detailed` | Compare all datasets with detailed output |
| `make clean` | Remove all generated files (results + expert folders) |
| `make help` | Show all commands and parameters |

### Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `TOP_K` | 10 | Number of top components per category |
| `MIN_VOTES` | 3 | Minimum expert votes for majority |
| `DATASET` | — | Dataset name (for single-dataset commands) |

---

## Expert Evaluation Workflow

1. **Run analysis** to generate templates:
   ```bash
   make analyze
   ```

2. **Distribute templates** — each expert copies the template and fills it in:
   ```bash
   cp experts/hub_application/template.txt experts/hub_application/expert_1.txt
   ```

3. **Fill in evaluations** — for each component, mark:
   - `E` = Structurally atypical (relative to other components in the system)
   - `H` = Structurally normal

4. **Compare** results with expert assessments:
   ```bash
   make compare-all-detailed
   ```

### Evaluation Criteria

Experts assess whether a component exhibits a **structurally atypical pattern
relative to other components** in the same system. This is **not** about
whether the component is "good" or "bad" — a central controller may be
intentionally designed as a hub, but it is still structurally atypical
compared to other applications.

---

## Output Format

### Per-Dataset Table
```
Component Type       K     Precision    Recall       F1-Score
----------------------------------------------------------------------
Application          5     0.60         0.75         0.67
                     10    0.40         0.80         0.53
```

### LaTeX Table (copy-paste ready)
```latex
\multirow{2}{*}{Application} & 5 & 0.60 & 0.75 & 0.67 \\
 & 10 & 0.40 & 0.80 & 0.53 \\
\hline
```

### Combined Results
When using `--all`, averaged metrics across all datasets are also produced.